#### Вопрос 04 

##### Что такое энтропия источника и как она определяется Энтропия. 

Среднее количество информации $H(A)$, которое приходится на одно сообщение, поступающее от источника без памяти, получим, применяя операцию усреднения по всему объему алфавита
$$
H(A)=-\sum\limits_{i=1}^kp(a_i)log_2\;p(a_i)
$$
Данное выражение известно как формула Шеннона для энтропии источника дискретных сообщений. Энтропия - мера неопределенности в поведении источника дискретных сообщений. Энтропия равна нулю, если с вероятностью единица источником выдается всегда одно и то же сообщение (в этом случае неопределенность в поведении источника сообщений отсутствует). Энтропия максимально, если символы источника появляются независимо и с одинаковой вероятностью.